#%% md


# Trabajo final de Aprendizaje No Supervisado
- **Coordinador**: Adrien Felipe
- **Secretaria**: Carolina Mart√≠nez
- **Revisor**: Enrique Navarro



#%% md

## Preparaci√≥n
### Librer√≠as

#%%

import itertools as it
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

# Para las medidas y algoritmos.
from sklearn import metrics
from sklearn.cluster import KMeans, DBSCAN, MeanShift, SpectralClustering
from sklearn.metrics import davies_bouldin_score
from sklearn.mixture import GaussianMixture
from scipy.cluster.hierarchy import linkage, cut_tree

# Eliminamos avisos molestos
import warnings

warnings.filterwarnings("ignore")

#%% md


### Funciones de apoyo
#### Funci√≥n de carga del dataset
Preparamos una funci√≥n gen√©rica para para simplificar la descarga de los datasets y su preparaci√≥n.     
√âsta nos permite escoger los atributos que usaremos, as√≠ como extraer a una variable las clases reales del dataset en caso de estar disponibles.    

Descripci√≥n de sus par√°metros:
 - dataset_url: cadena con la ruta al recurso desde donde cargar el dataset.
 - attributes: atributos del dataset a usar (pocisi√≥n y nombre).
 - separator (opcional): caracter de divisi√≥n en el origen del dataset.
 - class_position (opcional): ubicaci√≥n en el dataset de la clase.




#%%

def load_dataset(dataset_url: str, attributes: dict, separator: str = '\s+', class_position: int = None):
    """Load a dataset from a specified url into a pandas DataFrame.

    :param str dataset_url: an url from a text based dataset
    :param dict attributes: attributes to keep in dictionary form:
        key: attribute position, value: attribute name
    :param str separator: file separator.
    :param int class_position: column index where classes are defined (starts by 0)
        if left empty (None), no prediction class will be used (intrinsic case).
    """
    # Load dataset as a pandas DataFrame from a specified url.
    dataset = pd.read_csv(dataset_url, sep=separator, header=None)

    # Add class index to the indexes to extract.
    if class_position is not None:
        attributes[class_position] = 'classes'

    # Keep only desired attributes and classes.
    dataset = dataset[attributes]

    # Force all values to be numeric.
    for (column, values) in dataset.iteritems():
        # Do not transform classes.
        if column == class_position:
            continue

        # Coerce transforms non-numeric values into NaN.
        dataset[column] = pd.to_numeric(values, errors='coerce')

    # Remove all NaN rows.
    dataset.dropna(inplace=True)

    # Extrinsic case, dataset comes with its classes.
    if class_position is not None:
        # Extract classes.
        classes = dataset[class_position]
        # Remove classes from attributes.
        dataset.drop(class_position, axis=1, inplace=True)

    # Intrinsic case, dataset has no classes.
    else:
        classes = None

    # Set attributes title.
    dataset.rename(columns=attributes, inplace=True)

    return classes, dataset


#%% md


#### Funci√≥n de visualizaci√≥n
Usaremos una funci√≥n com√∫n para presentar los datos, tanto si est√°n clasificados como si no.       
Esta funci√≥n presenta una combinaci√≥n de dos en dos de todos los atributos, asi como adapta las dimensiones de la
gr√°fica seg√∫n la cantidad de sub-gr√°ficas a presentar.

Recibe dos par√°metros:
 - dataset: El DataFrame con los atributos a representar
 - classes (opcional): El DataFrame con la clase de cada instancia




#%%

def plot_dataset(dataset: pd.DataFrame, classes: np.array = None) -> None:
    # Combine all attributes two by two.
    combinations = list(it.combinations(dataset.columns, r=2))
    # Limit the number of plot columns.
    max_cols = 4
    cols = len(combinations) if len(combinations) <= max_cols else max_cols
    # From the columns number, set rows number.
    rows = int(np.ceil(len(combinations) / cols))

    # Calculate plot sizes depending on subplots number.
    size_x = int(13 * cols / max_cols) + 7
    size_y = 6 if rows * cols == 1 else 5 * rows

    # Build up all subplot combinations.
    fig, ax = plt.subplots(rows, cols, figsize=(size_x, size_y))
    for key, pair in enumerate(combinations):
        # Calculate plot axis position from sub-plot key.
        column = key % cols
        row = int(key / cols) % rows
        # Position needs to be a list when multiple rows.
        position = column if rows == 1 else (row, column)
        # Ax is not an array when single row and column.
        subplot = ax if rows * cols == 1 else ax[position]

        # Plot attributes values and titles.
        subplot.scatter(dataset[pair[0]], dataset[pair[1]], c=classes)
        subplot.set_title(str(pair[0]) + ' / ' + str(pair[1]))


#%% md


### Funciones propias de c√°lculo de medidas extr√≠nsecas:




#%%

def matriz_confusion(cat_real, cat_pred):
    cats = np.unique(cat_real)
    clusts = np.unique(cat_pred)
    mat = np.array([[np.sum(np.logical_and(cat_real == cats[i], cat_pred == clusts[j]))
                     for j in np.arange(clusts.size)]
                    for i in np.arange(cats.size)])
    return (mat)


def medida_error(mat):
    assign = np.sum([np.max(mat[l, :]) for l in np.arange(mat.shape[0])])
    return 1 - assign / float(np.sum(mat))


def medida_precision(mat, l, k):
    return mat[l, k] / sum(mat[:, k])


def medida_recall(mat, l, k):
    return mat[l, k] / sum(mat[l, :])


def medida_pureza(mat):
    totales = np.sum(mat, axis=0) / float(np.sum(mat))
    return np.sum([
        totales[k] * np.max(mat[:, k] / float(np.sum(mat[:, k])))
        for k in np.arange(mat.shape[1])
    ])


def medida_f1_especifica(mat, l, k):
    prec = medida_precision(mat, l, k)
    rec = medida_recall(mat, l, k)
    if (prec + rec) == 0:
        return 0
    else:
        return 2 * prec * rec / (prec + rec)


def medida_f1(mat):
    totales = np.sum(mat, axis=1) / float(np.sum(mat))
    assign = np.sum([
        totales[l] * np.max([
            medida_f1_especifica(mat, l, k)
            for k in np.arange(mat.shape[1])
        ])
        for l in np.arange(mat.shape[0])
    ])
    return assign


def medida_entropia(mat):
    totales = np.sum(mat, axis=0) / float(np.sum(mat))
    relMat = mat / np.sum(mat, axis=0)
    logRelMat = relMat.copy()
    logRelMat[logRelMat == 0] = 0.0001  # Evita el logaritmo de 0. Inofensivo pues luego desaparece al multiplicar por 0
    logRelMat = np.log(logRelMat)
    return -np.sum([
        totales[k] * np.sum([
            relMat[l, k] * logRelMat[l, k]
            for l in np.arange(mat.shape[0])
        ])
        for k in np.arange(mat.shape[1])
    ])


#%% md


#### Funci√≥n de c√°lculo de las medidas extr√≠nsecas
Con la intensi√≥n de simplificar y unificar la captura de las m√©tricas de valoraci√≥n aplicadas a cada algoritmo, preparamos una funci√≥n que calcula varias medidas cualitativas del agrupamiento, de forma a poder compararlas.    

√âsta aplica las siguiente m√©tricas:
 - Error, pureza, entrop√≠a, informaci√≥n mutua y F1 tal como se han visto en clase.
 - ARI mide la similaridad entre las clases y los predichos
 - Homogeneidad (todos los valores predichos son del cl√∫ster correcto)
 - Completaci√≥n (todos los valores de una clase se predicen en el mismo cl√∫ster)
 - Medida V (media arm√≥nica de homogeneidad y completaci√≥n). Par√°metro beta (por defecto 1) para ponderar
 - Fowlkes-Mallows es la media geom√©trica de las parejas precision-recall
 - Silhouette
 - Calinski-Harabasz
 - Davies-Bouldin

 Para simplificar la comparaci√≥n de resultados, se crea una media de algunos de los par√°metros, que son compatibles por
 puntuar con un m√°ximo de 1. Nos basaremos en ella para considerar qu√© algoritmo ofrece mejor resultado.




#%%

def calculate_extrinsic_metrics(dataset, real_classes, predicted_classes):
    confusion_matrix = matriz_confusion(real_classes, predicted_classes)

    return {
        'Error': medida_error(confusion_matrix),
        'Pureza': medida_pureza(confusion_matrix),
        'F1': medida_f1(confusion_matrix),
        'Entrop√≠a': medida_entropia(confusion_matrix),
        'Informaci√≥n m√∫tua': metrics.mutual_info_score(real_classes, predicted_classes),
        'ARI': metrics.adjusted_rand_score(real_classes, predicted_classes),
        'Homogeneidad': metrics.homogeneity_score(real_classes, predicted_classes),
        'Completaci√≥n': metrics.completeness_score(real_classes, predicted_classes),
        'Medida V': metrics.v_measure_score(real_classes, predicted_classes),
        'Fowlkes-Mallows': metrics.fowlkes_mallows_score(real_classes, predicted_classes),
        'Silhouette': metrics.silhouette_score(dataset, predicted_classes, metric='euclidean'),
        'Calinski-Harabasz': metrics.calinski_harabasz_score(dataset, predicted_classes),
        'Davies-Bouldin': davies_bouldin_score(dataset, predicted_classes),
        'media': (medida_pureza(confusion_matrix) + medida_f1(confusion_matrix) + metrics.mutual_info_score(
            real_classes, predicted_classes) + metrics.adjusted_rand_score(real_classes,
                                                                           predicted_classes) + metrics.homogeneity_score(
            real_classes, predicted_classes) + metrics.completeness_score(real_classes,
                                                                          predicted_classes) + metrics.v_measure_score(
            real_classes, predicted_classes) + metrics.fowlkes_mallows_score(real_classes, predicted_classes)) / 8
    }


#%% md


### Funciones de c√°lculo de medidas intr√≠nsecas
En el caso de la evaluci√≥n cualitativa del agrupamiento de los algoritmos intr√≠nsecas, usaremos exclusivamente m√©tricas disponibles en sklearn:
 - Silhouette
 - Calinski-Harabasz
 - Davies-Boudin

Debido a que RMSSTD, R¬≤ y la Medida I requieren de centroides, que no todos los algoritmos utilizan, decidimos prescindir de ellas.




#%%

def calculate_intrinsic_metrics(dataset, prediction):
    return {
        'Silhouette': metrics.silhouette_score(dataset, prediction),
        'Calinski Harabasz': metrics.calinski_harabasz_score(dataset, prediction),
        'Davies Bouldin': metrics.davies_bouldin_score(dataset, prediction)
    }


def r2_score(dataset, prediction, centroids):
    """
    An intrinsic R¬≤ score metric, as sklearn one is extrinsic only.
    """
    attributes_mean = np.mean(dataset, axis=0)
    labels = np.sort(np.unique(prediction))
    numerator = np.sum([
        np.sum(np.sum(dataset[prediction == label] - centroids[label], axis=1) ** 2)
        for label in labels
    ])
    denominator = np.sum(np.sum(dataset - attributes_mean, 1) ** 2)

    return 1 - numerator / denominator


#%% md


### Funci√≥n para presentar las m√©tricas

Finalmente se crea una funci√≥n que simplifique la comparaci√≥n de m√©tricas entre distintos algoritmos.




#%%

def compare_metrics(metrics_data: dict) -> pd.DataFrame:
    output = pd.DataFrame(metrics_data)
    return output


#%% md


Funci√≥n para generar gr√°ficamente la evoluci√≥n de las m√©tricas R¬≤ y Silueta seg√∫n el n√∫mero de clusters, para poder escoger el n√∫mero de clusters √≥ptimo usando la t√©cnica del codo.




#%%

def plot_clusters_selection(dataset: pd.DataFrame, max_clusters: int = 10):
    dataset = np.array(dataset)
    silhouette_values = []
    r2_values = []
    min_clusters = 2

    for k in np.arange(min_clusters, max_clusters):
        model = KMeans(n_clusters=k).fit(dataset)
        prediction = model.predict(dataset)
        centroids = model.cluster_centers_

        silhouette_values += [metrics.silhouette_score(dataset, prediction)]
        r2_values += [r2_score(dataset, prediction, centroids)]

    fig, ax = plt.subplots(1, 2, figsize=(15, 5))
    ax[0].plot(np.arange(min_clusters, max_clusters), silhouette_values, linestyle='-', marker='o')
    ax[0].set_xlabel("N√∫mero de cl√∫steres")
    ax[0].set_ylabel("Medida de ancho de silueta")

    ax[1].plot(np.arange(min_clusters, max_clusters), r2_values, linestyle='-', marker='o')
    ax[1].set_xlabel("N√∫mero de cl√∫steres")
    ax[1].set_ylabel("Medida de R cuadrado")


#%% md


# Dataset extr√≠nseca

El origen de este dataset se remonta a datos usados en 1983 por la <i>American Statistical Association Exposition</i> y que se conservan en la Universidad de Carnegie Mellon, al que le faltan 8 instancias que se eliminaron para homogeneizar el dataset, ya que carec√≠an del campo mpg.

 El dataset consta de:
 - 392 instancias
 - 8 atributos:    
     - mpg (millas por gal√≥n de combustible): de tipo continuo.    
     - cylinders (cilindros): discreto multi evaluado.    
     - displacement (cilindrada): continuo.    
     - horsepower (caballos de potencia): continuo.    
     - weight (peso): continuo    
     - acceleration (aceleraci√≥n): continuo    
     - model-year (a√±o del modelo): discrto multi evaluado.    
     - origin (origen): discreto multi evaluado.    
     - car name (nombre del coche): cadena (√∫nico para cada instancia)    

Para el estudio que nos ocupa vamos a predecir el n√∫mero de cilindros bas√°ndonos en el consumo, la cilindrada y la potencia.



#%%

# Cargamos el dataset.
dataset_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'
attributes = {0: 'mpg', 2: 'cilindrada', 3: 'potencia'}
extrinsic_classes, extrinsic_dataset = load_dataset(dataset_url, attributes, class_position=1)

# Soporte para las m√©tricas
extrinsic_metrics = {}

#%% md


Se descartan el resto de valores para mantener baja la dimensi√≥n del vector descriptor y simplificar as√≠ los c√°lculos.
Los datos vienen casi listos para trabajar con ellos. No se detectan campos vac√≠os:



#%%

print(extrinsic_dataset.isnull().any())

#%% md


Sin embargo, en la potencia existe un valor an√≥malo, un "?" usado donde se desconoc√≠a el dato, por lo que se ha incorporado a la funci√≥n de carga de datos un filtro para eliminarlo.

Vamos a observar la distribuci√≥n de nuestra clase:



#%%

sns.distplot(extrinsic_classes)

#%% md


Se observa una marcada preponderancia de los valores de cilindros 4,6 y 8. Asumimos que probablemente las instancias que no pertenezcan a estos tres grupos se agrupen dentro de ellos lo que va a conllevar un peque√±o error de base al escoger agrupamientos.

Y la relaci√≥n entre los atributos:




#%%

plot_dataset(extrinsic_dataset, extrinsic_classes)

#%% md


## An√°lisis dataset extr√≠nseca

Observando los datos es evidente que el n√∫mero √≥ptimo de cl√∫sters para K-means es 3.

Definimos un variable con el n√∫mero de cluster que usaremos para el an√°lisis:



#%%

extrinsic_clusters = 3

#%% md

### Algoritmo 1: K medias

#%%

# Generamos el modelo.
model = KMeans(n_clusters=extrinsic_clusters).fit(extrinsic_dataset)
prediction = model.predict(extrinsic_dataset)

# Guardamos la m√©tricas.
extrinsic_metrics['k-means'] = calculate_extrinsic_metrics(extrinsic_dataset, extrinsic_classes, prediction)

# Presentamos los clusters.
plot_dataset(extrinsic_dataset, prediction)

#%% md

### Algoritmo 2: jer√°rquico aglomerativo

#%%

# Generamos el modelo.
model = linkage(extrinsic_dataset, 'average')
prediction = cut_tree(model, n_clusters=extrinsic_clusters).flatten()
# Guardamos la m√©tricas.
extrinsic_metrics['Jer√°rquico'] = calculate_extrinsic_metrics(extrinsic_dataset, extrinsic_classes, prediction)

# Presentamos los clusters.
plot_dataset(extrinsic_dataset, prediction)


#%% md

### Algoritmo 3: DBSCAN

#%%

def calcular_DBSCAN(eps):
    modelo = DBSCAN(eps=eps).fit(extrinsic_dataset)
    labels_pred = modelo.labels_
    x = calculate_extrinsic_metrics(extrinsic_dataset, extrinsic_classes, labels_pred)
    media = (x['ARI'] + x['Informaci√≥n m√∫tua'] + x['Homogeneidad'] + x['Completaci√≥n'] + x['Medida V'] + x[
        'Fowlkes-Mallows']) / 6
    return {"modelo": modelo, "mediciones": x, 'media': media, "prediction": labels_pred}


def repetir_dbscan(r):
    r *= 2
    res = {"media": 0}
    for i in np.arange(20 * 2, r + 1):
        x = calcular_DBSCAN(i / 2)
        if x["media"] > res["media"]:
            res = x
            res["distancia"] = i / 2
    return res


eps = 30  # Distancia m√°xima a probar (en pasos de 0.5)
best = repetir_dbscan(eps)
print("Mejor distancia identificada:", best["distancia"])
extrinsic_metrics["DBSCAN"] = best["mediciones"]

plot_dataset(extrinsic_dataset, best["prediction"])

#%% md

### Algoritmo 4: Deslizamiento de media

#%%

# Generamos el modelo.
model = MeanShift().fit(extrinsic_dataset)
prediction = model.labels_

# Guardamos la m√©tricas.
extrinsic_metrics['Means-Shift'] = calculate_extrinsic_metrics(extrinsic_dataset, extrinsic_classes, prediction)

# Presentamos los clusters.
plot_dataset(extrinsic_dataset, prediction)


#%% md

### Algritmo 5: Espectral

#%%

def mejor_espectral(nn):
    vecinos = 0
    media_max = 0
    modelo_fin = None
    for i in np.arange(nn):
        modelo = SpectralClustering(affinity='nearest_neighbors', n_neighbors=i + 1).fit(extrinsic_dataset)
        labels_pred = modelo.labels_
        x = calculate_extrinsic_metrics(extrinsic_dataset, extrinsic_classes, labels_pred)
        x['media'] = (x['ARI'] + x['Informaci√≥n m√∫tua'] + x['Homogeneidad'] + x['Completaci√≥n'] + x['Medida V'] + x[
            'Fowlkes-Mallows']) / 6
        if x["media"] > media_max:
            vecinos = i + 1
            media_max = x["media"]
            modelo_fin = modelo
    return {"modelo": modelo_fin, "vecinos": vecinos, "mediciones": x, "prediction": labels_pred}


def repetir_espectral(v, r):
    print("Buscando mejor clustering espectral.\nProbando de 1 a", v, "vecinos m√°s cercanos y repitiendo", r,
          "veces.\nTiempo de ejecuci√≥n estimado:", int((v / 53) * 3 * r), "segundos.")
    mejor = {"mediciones": {"media": 0}}
    for i in np.arange(r + 1):
        res = mejor_espectral(v)
        if res["mediciones"]["media"] > mejor["mediciones"]["media"]:
            mejor = res
    return mejor


vecinos = 30
repeticiones = 10
best = repetir_espectral(vecinos, repeticiones)
print("El mejor espectral encontrado es con", best["vecinos"], "vecinos y da una media de", best["mediciones"]["media"])
# for key, value in best["mediciones"].items():
#     print(key, ":", value)
extrinsic_metrics["Espectral"] = best["mediciones"]

plot_dataset(extrinsic_dataset, best["prediction"])

#%% md

## Comparaci√≥n algoritmos


Vamos pues a obtener una comparativa de los algoritmos para nuestro dataset extr√≠nseco:


#%%

display(compare_metrics(extrinsic_metrics))

#%% md


Por lo que se observa, bas√°ndonos en la media calculada, el <b>mejor algoritmo para el agrupamiento de nuestros datos es el de agrupamiento jer√°rquico, pr√°cticamente igualado a K Medias</b>, seguidos por desplazamiento de medias y DBSCAN.

El espectral, en cambio, no resulta muy apropiado para este caso.


#%% md


# Dataset intr√≠nseca
El dataset intr√≠nseca **Aggregations** est√° generado de manera artificial por: *A. Gionis, H. Mannila, and P. Tsaparas, Clustering aggregation. ACM Transactions on Knowledge Discovery from Data (TKDD), 2007*


Este dataset est√° compuesto por 788 observaciones de 2 variables que abarcan un amplio rango num√©rico. En el conjunto de datos existen entre 5 a 7 grupos que se distribuyen en zonas particulares del rango de valores de las variables.


Cargamos nuestro dataset (*intrinsic_dataset*):



#%%

# Cargamos el dataset.
dataset_url = 'http://cs.joensuu.fi/sipu/datasets/Aggregation.txt'
attributes = {0: 'dim 1', 1: 'dim 2'}
_, intrinsic_dataset = load_dataset(dataset_url, attributes)

# Soporte para las m√©tricas
intrinsic_metrics = {}

#%% md


Visualizamos el dataset en 2-D:



#%%

plot_dataset(intrinsic_dataset)

#%% md


Observando las caracter√≠sticas de esta representaci√≥n, podemos decir que es un conjunto de datos compacto, 
lo que nos permitir√° obtener resultados aceptables con con algoritmos de agrupamiento K-means y jer√°rquicos, 
y parece que se podr√≠a clasificar con 4, 5 o con 7 clusters.



#%% md

## An√°lisis dataset intr√≠nseca

#%% md


### Selecci√≥n del n√∫mero de clusters

A fin de implementar el modelo de K-Medios, comencemos por determinar la cantidad √≥ptima de centroides a utilizar a partir del M√©todo del Codo.



#%%

plot_clusters_selection(intrinsic_dataset)

#%% md


Seg√∫n el procedimiento del codo, escoger√≠amos entre 5 y 7 clusters. Aunque estos valores son para escoger la cantidad √≥ptima de centroides, son los valores sobre los que hemos realizado el an√°lisis de todos los algoritmos utilizados.

Usamos un variable con el n√∫mero buscado de clusters:



#%%

intrinsic_clusters = 7

#%% md



### Algoritmo 1: K medias
Durante el an√°lisis ejecutamos la predicci√≥n de k-means con 5, 6 y 7 clusters, y finalmente ejecutamos y visualizamos la agrupaci√≥n generada para K = 7.



#%%

# Generamos el modelo.
model = KMeans(n_clusters=intrinsic_clusters).fit(intrinsic_dataset)
prediction = model.predict(intrinsic_dataset)

# Guardamos la m√©tricas.
intrinsic_metrics['k-means'] = calculate_intrinsic_metrics(intrinsic_dataset, prediction)

# Presentamos los clusters.
plot_dataset(intrinsic_dataset, prediction)

#%% md


Vemos que buena parte de los grupos se han identificado correctamente, o con m√≠nimas interferencias (el grupo amarillo \"invade\" al azul oscuro en dos puntos). Sin embargo, los dos grupos peque√±os de abajo a la izquierda los considera uno, junto con algunos puntos del grupo grande a su lado, que a su vez est√° dividido en dos.



#%% md

### Algoritmo 2: Jer√°rquico Aglomerativo

#%%

# Generamos el modelo.
model = linkage(intrinsic_dataset, 'average')
prediction = cut_tree(model, n_clusters=intrinsic_clusters).flatten()

# Guardamos la m√©tricas.
intrinsic_metrics['Jer√°rquico'] = calculate_intrinsic_metrics(intrinsic_dataset, prediction)

# Presentamos los clusters.
plot_dataset(intrinsic_dataset, prediction)

#%% md


El resultado de este algoritmo de agrupamiento es excelente, acertando completamente los 7 grupos que se adivinan visualmente.


#%% md

### Algoritmo 3: Agrupamiento espectral

#%%

# Generamos el modelo.
knn = 34
model = SpectralClustering(
    n_clusters=intrinsic_clusters, affinity='nearest_neighbors', n_neighbors=knn, random_state=0
).fit(intrinsic_dataset)
prediction = model.labels_

# Guardamos la m√©tricas.
intrinsic_metrics['Espectral'] = calculate_intrinsic_metrics(intrinsic_dataset, prediction)

# Presentamos los clusters.
plot_dataset(intrinsic_dataset, prediction)

#%% md


El jer√°rquico con 30 KNN resuelve bien la clusterizaci√≥n con 7 grupos, si se reduce no lo hace tan bien, y  partir de 50 tampoco. Hay que encontar el valor correcto.    
Buscando 5 clusters tambi√©n lo hace bien.



#%% md

### Algoritmo 4: Mean Shift

#%%

# Generamos el modelo.
h = 4
model = MeanShift(bandwidth=h).fit(intrinsic_dataset)
prediction = model.labels_

# Guardamos la m√©tricas.
intrinsic_metrics['Means-Shift'] = calculate_intrinsic_metrics(intrinsic_dataset, prediction)

# Presentamos los clusters.
plot_dataset(intrinsic_dataset, prediction)

#%% md


Este algoritmo resuelve casi correctamente el agrupamiento, identificando los 7 grupos pero asignando mal algunos puntos, incluyendo en los grupos peque√±os puntos de los grupos grandes m√°s cercanos. Es un problema conocido del algoritmo, al trabajar sobre una media general para todos los agrupamientos.


#%% md

### Algoritmo 5: EM

#%%

# Generamos el modelo.
model = GaussianMixture(n_components=intrinsic_clusters, max_iter=1000, random_state=8).fit(intrinsic_dataset)
prediction = model.predict(intrinsic_dataset)

# Guardamos la m√©tricas.
intrinsic_metrics['EM'] = calculate_intrinsic_metrics(intrinsic_dataset, prediction)

# Presentamos los clusters.
plot_dataset(intrinsic_dataset, prediction)

#%% md


Este algoritmo tiene una gran variabilidad, resultando √≥ptimo en ocasiones y alej√°ndose de ese resultado en otras.     
Hemos dejado fijado el *random state* de tal forma que visualicemos un caso de agrupaci√≥n perfecta, aunque en la mayor√≠a de los casos no logra este nivel de agrupaci√≥n.


#%% md

## Comparaci√≥n algoritmos

#%%

display(pd.DataFrame(intrinsic_metrics))

#%% md


Inicialmente se analizan los algoritmos con un n√∫mero de cl√∫sters distinto para cada uno, sin embargo de cara a la presentaci√≥n final y conclusiones, escogemos la misma cantidad de grupos para poder comparar y en coherencia con lo que nos dice la visualizaci√≥n del dataset. Este valor ser√° **7**, el cual adem√°s iguala bastante los indicadores.

En las predicciones podemos obeservar las siguientes curiosidades. 
> Entre el *Jer√°rquico aglomerativo* y el *Espectral* podemos observar en el resultado que s√≥lo se diferencia en los puntos de m√≠nima distancia inter-cl√∫ster, lo corroboran los indicadores de ambos, que son valores muy similares.

> Los siguientes indicadores similares son *K-Means* y *EM*, principalmente la silueta, y aunque la predicci√≥n de cl√∫sters no coincide si que se observa que la separaci√≥n entre muestras y cl√∫sters vecinos ser√° parecido, tal y como divide en m√°s de un grupo el conjunto de datos de mayor volumen inferior.

Seg√∫n el **coeficiente de silueta** tanto con el *Jer√°rquico* como con el *Means-Shift* obtenemos muestras m√°s separadas de otros cl√∫sters vecinos. Pero los valores en el resto de algoritmos son bastante parecidos, y por tanto no es una caracter√≠stica claramente diferenciadora.

Sin embargo, el indicador **Calinski-Harabasz** que relaciona la cohesi√≥n y separaci√≥n de la siguiente forma: $\frac{ùëÜùëÜùêµ/(ùëò‚àí1)}
{ùëÜùëÜùëä/(ùëõ‚àíùëò)}$, nos da como mejor resultado de clasificaci√≥n el obtenido con el algoritmo *K-Means*, seguido del *EM*, esto tiene sentido ya que CH tiende a preferir soluciones de agrupaci√≥n con agrupaciones que consisten en aproximadamente el mismo n√∫mero de objetos.

Finalmente el √≠ndice **Davies Bouldin**, se√±ala tanto al *Jer√°rquico* como al *Espectral* como los mejores agrupamientos, esto es debido a que se mide la proporci√≥n entre la suma de la dispersi√≥n dentro del cl√∫ster a la separaci√≥n entre cl√∫sters, y por tanto apremia a resultados de agrupamiento en los que no tienen por qu√© ser similares los grupos entre si. Esto coincide con lo que nos dice nuestra intuici√≥n observando el conjunto de datos. 



#%% md


# Conclusi√≥n
Veamos los resultados de uno y otro an√°lisis:




#%%

def simplificar_extrinsic():
    e_m = []
    for key in extrinsic_metrics:
        k = {}
        for metrica in extrinsic_metrics[key]:
            if metrica in ['Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin']:
                k[metrica] = extrinsic_metrics[key][metrica]
        e_m.append(k)
    e_m_df = pd.DataFrame(e_m).transpose()
    e_m_df.columns = ['k-means', 'Jer√°rquico', 'DBSCAN', 'Means-Shift', 'Espectral']
    col_list = list(e_m_df)
    col_list[2], col_list[4] = col_list[4], col_list[2]
    e_m_df.columns = col_list
    return e_m_df


print("Resultados del dataset intr√≠nseco:")
display(pd.DataFrame(intrinsic_metrics))
print("Resultados del dataset extr√≠nseco:")
display(simplificar_extrinsic())

#%% md


Se aprecia una diferencia sustancial entre los resultados seg√∫n si se conoce o no el valor de la clase, muy homog√©neos en caso de Silhouette y con una sola excepci√≥n (el Espectral) para la m√©trica de Calinski Harabasz.

Sin embargo, la m√©trica Davis-Bouldin parece bastante independiente del hecho de que el dataset sea intr√≠nseco o extr√≠nseco.

Para ambos conjuntos de datos el algoritmo jer√°rquico aglomerativo ha dado un buen resultado.

En esta pr√°ctica vimos c√≥mo es posible implementar distintos agrupamientos sobre el mismo conjunto de datos bidimensionales, y c√≥mo estos algoritmos son capaces tambi√©n de encontrar las semejanzas intr√≠nsecas de los datos y producir clases que, en efecto, reproducen lo que puede observarse de manera intuitiva a partir de la gr√°fica de los datos.


